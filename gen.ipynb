{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Image Captioning\n",
    "# [{\n",
    "# \"image_id\": int, \n",
    "#  \"caption\": str,\n",
    "# }]\n",
    "\n",
    "# results.zip\n",
    "#   captions_val2014_[alg]_results.json\n",
    "#   captions_test2014_[alg]_results.json\n",
    "#  Replace [alg] with your algorithm name and place both files into a single zip file named \"results.zip\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"  # specify which GPU(s) to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from data_loader import get_loader\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from processData import Vocabulary\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import skimage.transform\n",
    "import argparse\n",
    "from scipy.misc import imread, imresize\n",
    "from PIL import Image\n",
    "import matplotlib.image as mpimg\n",
    "from IPython import display\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "from scipy import misc\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = pickle.load(open('glove.6B/glove_words.pkl', 'rb'))\n",
    "glove_vectors = torch.tensor(glove_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Encoder RASNET CNN - pretrained\n",
    "#####################################\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        resnet = models.resnet101(pretrained=True)\n",
    "        self.resnet = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((14, 14))\n",
    "\n",
    "    def forward(self, images):\n",
    "        out = self.adaptive_pool(self.resnet(images))\n",
    "        # batch_size, img size, imgs size, 2048\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Attention Decoder\n",
    "####################\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, use_glove, use_bert):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.encoder_dim = 2048\n",
    "        self.attention_dim = 512\n",
    "        self.use_bert = use_bert\n",
    "        if use_glove:\n",
    "            self.embed_dim = 300\n",
    "        elif use_bert:\n",
    "            self.embed_dim = 768\n",
    "        else:\n",
    "            self.embed_dim = 512\n",
    "            \n",
    "        self.decoder_dim = 512\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = 0.5\n",
    "        \n",
    "        # soft attention\n",
    "        self.enc_att = nn.Linear(2048, 512)\n",
    "        self.dec_att = nn.Linear(512, 512)\n",
    "        self.att = nn.Linear(512, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        # decoder layers\n",
    "        self.dropout = nn.Dropout(p=self.dropout)\n",
    "        self.decode_step = nn.LSTMCell(self.embed_dim + self.encoder_dim, self.decoder_dim, bias=True)\n",
    "        self.h_lin = nn.Linear(self.encoder_dim, self.decoder_dim)\n",
    "        self.c_lin = nn.Linear(self.encoder_dim, self.decoder_dim)\n",
    "        self.f_beta = nn.Linear(self.decoder_dim, self.encoder_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc = nn.Linear(self.decoder_dim, self.vocab_size)\n",
    "\n",
    "        # init variables\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "        \n",
    "        if not use_bert:\n",
    "            self.embedding = nn.Embedding(vocab_size, self.embed_dim)\n",
    "            self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "            # load Glove embeddings\n",
    "            if use_glove:\n",
    "                self.embedding.weight = nn.Parameter(glove_vectors)\n",
    "\n",
    "            # always fine-tune embeddings (even with GloVe)\n",
    "            for p in self.embedding.parameters():\n",
    "                p.requires_grad = True\n",
    "            \n",
    "\n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):    \n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(-1)\n",
    "        vocab_size = self.vocab_size\n",
    "        dec_len = [x-1 for x in caption_lengths]\n",
    "        max_dec_len = max(dec_len)\n",
    "\n",
    "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "\n",
    "        if not self.use_bert:\n",
    "            embeddings = self.embedding(encoded_captions)\n",
    "        elif self.use_bert:\n",
    "            embeddings = []\n",
    "            for cap_idx in  encoded_captions:\n",
    "                \n",
    "                # padd caption to correct size\n",
    "                while len(cap_idx) < max_dec_len:\n",
    "                    cap_idx.append(PAD)\n",
    "                    \n",
    "                cap = ' '.join([vocab.idx2word[word_idx.item()] for word_idx in cap_idx])\n",
    "                cap = u'[CLS] '+cap\n",
    "                \n",
    "                tokenized_cap = tokenizer.tokenize(cap)                \n",
    "                indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_cap)\n",
    "                tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    encoded_layers, _ = model(tokens_tensor)\n",
    "\n",
    "                bert_embedding = encoded_layers[11].squeeze(0)\n",
    "                \n",
    "                split_cap = cap.split()\n",
    "                tokens_embedding = []\n",
    "                j = 0\n",
    "\n",
    "                for full_token in split_cap:\n",
    "                    curr_token = ''\n",
    "                    x = 0\n",
    "                    for i,_ in enumerate(tokenized_cap[1:]): # disregard CLS\n",
    "                        token = tokenized_cap[i+j]\n",
    "                        piece_embedding = bert_embedding[i+j]\n",
    "                        \n",
    "                        # full token\n",
    "                        if token == full_token and curr_token == '' :\n",
    "                            tokens_embedding.append(piece_embedding)\n",
    "                            j += 1\n",
    "                            break\n",
    "                        else: # partial token\n",
    "                            x += 1\n",
    "                            \n",
    "                            if curr_token == '':\n",
    "                                tokens_embedding.append(piece_embedding)\n",
    "                                curr_token += token.replace('#', '')\n",
    "                            else:\n",
    "                                tokens_embedding[-1] = torch.add(tokens_embedding[-1], piece_embedding)\n",
    "                                curr_token += token.replace('#', '')\n",
    "                                \n",
    "                                if curr_token == full_token: # end of partial\n",
    "                                    j += x\n",
    "                                    break                            \n",
    "                \n",
    "                               \n",
    "                cap_embedding = torch.stack(tokens_embedding)\n",
    "\n",
    "                embeddings.append(cap_embedding)\n",
    "                \n",
    "            embeddings = torch.stack(embeddings)\n",
    "\n",
    "        # init hidden state\n",
    "        avg_enc_out = encoder_out.mean(dim=1)\n",
    "        h = self.h_lin(avg_enc_out)\n",
    "        c = self.c_lin(avg_enc_out)\n",
    "\n",
    "        predictions = torch.zeros(batch_size, max_dec_len, vocab_size)\n",
    "        alphas = torch.zeros(batch_size, max_dec_len, num_pixels)\n",
    "\n",
    "        for t in range(max(dec_len)):\n",
    "            batch_size_t = sum([l > t for l in dec_len ])\n",
    "            \n",
    "            # soft-attention\n",
    "            enc_att = self.enc_att(encoder_out[:batch_size_t])\n",
    "            dec_att = self.dec_att(h[:batch_size_t])\n",
    "            att = self.att(self.relu(enc_att + dec_att.unsqueeze(1))).squeeze(2)\n",
    "            alpha = self.softmax(att)\n",
    "            attention_weighted_encoding = (encoder_out[:batch_size_t] * alpha.unsqueeze(2)).sum(dim=1)\n",
    "        \n",
    "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            \n",
    "            batch_embeds = embeddings[:batch_size_t, t, :]  \n",
    "            cat_val = torch.cat([batch_embeds.double(), attention_weighted_encoding.double()], dim=1)\n",
    "            \n",
    "            h, c = self.decode_step(cat_val.float(),(h[:batch_size_t].float(), c[:batch_size_t].float()))\n",
    "            preds = self.fc(self.dropout(h))\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "            alphas[:batch_size_t, t, :] = alpha\n",
    "            \n",
    "        # preds, sorted capts, dec lens, attention wieghts\n",
    "        return predictions, encoded_captions, dec_len, alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.64s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.42s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# loss\n",
    "class loss_obj(object):\n",
    "    def __init__(self):\n",
    "        self.avg = 0.\n",
    "        self.sum = 0.\n",
    "        self.count = 0.\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "PAD = 0\n",
    "START = 1\n",
    "END = 2\n",
    "UNK = 3\n",
    "\n",
    "grad_clip = 5.\n",
    "num_epochs = 1\n",
    "batch_size = 32\n",
    "decoder_lr = 0.0004\n",
    "# Load vocabulary wrapper\n",
    "with open('data/vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "# load data\n",
    "train_loader = get_loader('train', vocab, batch_size)\n",
    "val_loader = get_loader('val', vocab, batch_size)\n",
    "test_loader = get_loader('test', vocab, batch_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8857"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#############\n",
    "# New model\n",
    "#############\n",
    "\n",
    "# decoder = Decoder(vocab_size=len(vocab),use_glove=False, use_bert=True) # true = use glove\n",
    "# decoder_optimizer = torch.optim.Adam(params=decoder.parameters(),lr=decoder_lr)\n",
    "\n",
    "# encoder = Encoder()\n",
    "\n",
    "#############\n",
    "# Load model\n",
    "#############\n",
    "\n",
    "encoder = Encoder()\n",
    "encoder_checkpoint = torch.load('./checkpoints_baseline1/encoder_baseline',map_location='cpu')\n",
    "encoder.load_state_dict(encoder_checkpoint['model_state_dict'])\n",
    "decoder = Decoder(vocab_size=len(vocab),use_glove=False, use_bert=False)\n",
    "decoder_optimizer = torch.optim.Adam(params=decoder.parameters(),lr=decoder_lr)\n",
    "decoder_checkpoint = torch.load('./checkpoints_baseline1/decoder_baseline',map_location='cpu')\n",
    "decoder.load_state_dict(decoder_checkpoint['model_state_dict'])\n",
    "decoder_optimizer.load_state_dict(decoder_checkpoint['optimizer_state_dict'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    print(\"Started training...\")\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        decoder.train()\n",
    "        encoder.train()\n",
    "\n",
    "        losses = loss_obj()\n",
    "        num_batches = len(train_loader)\n",
    "\n",
    "        for i, (imgs, caps, caplens) in enumerate(tqdm(train_loader)):\n",
    "            if i > 20:\n",
    "                break\n",
    "\n",
    "            imgs = encoder(imgs)\n",
    "\n",
    "            scores, caps_sorted, decode_lengths, alphas = decoder(imgs, caps, caplens)\n",
    "            scores = pack_padded_sequence(scores, decode_lengths, batch_first=True)[0]\n",
    "\n",
    "            targets = caps_sorted[:, 1:]\n",
    "            targets = pack_padded_sequence(targets, decode_lengths, batch_first=True)[0]\n",
    "\n",
    "            loss = criterion(scores, targets)\n",
    "            loss += ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
    "\n",
    "            decoder_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # grad_clip decoder\n",
    "            for group in decoder_optimizer.param_groups:\n",
    "                for param in group['params']:\n",
    "                    if param.grad is not None:\n",
    "                        param.grad.data.clamp_(-grad_clip, grad_clip)\n",
    "\n",
    "            decoder_optimizer.step()\n",
    "\n",
    "            losses.update(loss.item(), sum(decode_lengths))\n",
    "            print('Batch '+str(i)+'/'+str(num_batches)+' loss:'+str(losses.avg))\n",
    "\n",
    "            # save model each 100 batches\n",
    "            if i%60==0 and i!=0:\n",
    "                print('Batch '+str(i)+'/'+str(num_batches)+' loss:'+str(losses.avg))\n",
    "                \n",
    "                 # adjust learning rate (create condition for this)\n",
    "                for param_group in decoder_optimizer.param_groups:\n",
    "                    param_group['lr'] = param_group['lr'] * 0.5\n",
    "\n",
    "                print('saving model...')\n",
    "\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': decoder.state_dict(),\n",
    "                    'optimizer_state_dict': decoder_optimizer.state_dict(),\n",
    "                    'loss': loss,\n",
    "                    }, './checkpoints/decoder_4')\n",
    "\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': encoder.state_dict(),\n",
    "                    'loss': loss,\n",
    "                    }, './checkpoints/encoder_4')\n",
    "\n",
    "                print('model saved')\n",
    "        \n",
    "        print('Epoch Loss:'+str(losses.avg))\n",
    "\n",
    "    print(\"Completed training...\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def gen(hypotheses,references,img_ids, alphas, k, gt):\n",
    "    print('Baseline Model')\n",
    "    img_dim = 500 # 14*24\n",
    "    \n",
    "    hyp_sentence = []\n",
    "    for word_idx in hypotheses[k]:\n",
    "        hyp_sentence.append(vocab.idx2word[word_idx])\n",
    "    \n",
    "    ref_sentence = []\n",
    "    for word_idx in references[k]:\n",
    "        ref_sentence.append(vocab.idx2word[word_idx])\n",
    "        \n",
    "    print('Hypotheses: '+\" \".join(hyp_sentence))\n",
    "    print('References: '+\" \".join(ref_sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [] \n",
    "test_references = []\n",
    "hypotheses = [] \n",
    "all_imgs = []\n",
    "all_alphas = []\n",
    "\n",
    "def validate():\n",
    "    print(\"Started validation...\")\n",
    "    decoder.eval()\n",
    "    encoder.eval()\n",
    "\n",
    "    losses = loss_obj()\n",
    "\n",
    "    num_batches = len(val_loader)\n",
    "    print('Batches',num_batches)\n",
    "    # Batches\n",
    "    for i, (imgs, caps, caplens) in enumerate(tqdm(val_loader)):\n",
    "        if i > 0:\n",
    "            break\n",
    "\n",
    "        imgs_jpg = imgs.numpy() \n",
    "        imgs_jpg = np.swapaxes(np.swapaxes(imgs_jpg, 1, 3), 1, 2)\n",
    "        \n",
    "        # Forward prop.\n",
    "        imgs = encoder(imgs)\n",
    "        scores, caps_sorted, decode_lengths, alphas = decoder(imgs, caps, caplens)\n",
    "        targets = caps_sorted[:, 1:]\n",
    "\n",
    "        # Remove timesteps that we didn't decode at, or are pads\n",
    "        scores_packed = pack_padded_sequence(scores, decode_lengths, batch_first=True)[0]\n",
    "        targets_packed = pack_padded_sequence(targets, decode_lengths, batch_first=True)[0]\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(scores_packed, targets_packed)\n",
    "        loss += ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
    "\n",
    "        losses.update(loss.item(), sum(decode_lengths))\n",
    "\n",
    "         # References\n",
    "        for j in range(targets.shape[0]):\n",
    "            img_caps = targets[j].tolist() # validation dataset only has 1 unique caption per img\n",
    "            clean_cap = [w for w in img_caps if w not in [PAD, START, END]]  # remove pad, start, and end\n",
    "            img_captions = list(map(lambda c: clean_cap,img_caps))\n",
    "            test_references.append(clean_cap)\n",
    "            references.append(img_captions)\n",
    "\n",
    "        # Hypotheses\n",
    "        _, preds = torch.max(scores, dim=2)\n",
    "        preds = preds.tolist()\n",
    "        temp_preds = list()\n",
    "        for j, p in enumerate(preds):\n",
    "            pred = p[:decode_lengths[j]]\n",
    "            pred = [w for w in pred if w not in [PAD, START, END]]\n",
    "            temp_preds.append(pred)  # remove pads, start, and end\n",
    "        preds = temp_preds\n",
    "        hypotheses.extend(preds)\n",
    "        \n",
    "        all_alphas.append(alphas)\n",
    "        all_imgs.append(imgs_jpg)\n",
    "    print('ref',len(references))\n",
    "    print('hy',len(hypotheses))\n",
    "\n",
    "    bleu = corpus_bleu(references, hypotheses)\n",
    "    bleu_1 = corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0))\n",
    "    bleu_2 = corpus_bleu(references, hypotheses, weights=(0, 1, 0, 0))\n",
    "    bleu_3 = corpus_bleu(references, hypotheses, weights=(0, 0, 1, 0))\n",
    "    bleu_4 = corpus_bleu(references, hypotheses, weights=(0, 0, 0, 1))\n",
    "\n",
    "\n",
    "    print(\"Validation loss: \"+str(losses.avg))\n",
    "    print(\"BLEU: \"+str(bleu))\n",
    "    print(\"BLEU-1: \"+str(bleu_1))\n",
    "    print(\"BLEU-2: \"+str(bleu_2))\n",
    "    print(\"BLEU-3: \"+str(bleu_3))\n",
    "    print(\"BLEU-4: \"+str(bleu_4))\n",
    "    print_sample(hypotheses, test_references, all_imgs, all_alphas,1,True)\n",
    "    print(\"Completed validation...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (imgs, caps, caplens,img_ids) in enumerate(tqdm(val_loader)):\n",
    "    if i > 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_ids[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def compare_sample(references,bert_hypotheses,glove_hypotheses, baseline_hypotheses, imgs, k):\n",
    "    img_dim = 500 # 14*24\n",
    "    \n",
    "    bert_hyp_sentence = []\n",
    "    for word_idx in bert_hypotheses[k]:\n",
    "        bert_hyp_sentence.append(vocab.idx2word[word_idx])\n",
    "        \n",
    "    glove_hyp_sentence = []\n",
    "    for word_idx in glove_hypotheses[k]:\n",
    "        glove_hyp_sentence.append(vocab.idx2word[word_idx])\n",
    "    \n",
    "    baseline_hyp_sentence = []\n",
    "    for word_idx in baseline_hypotheses[k]:\n",
    "        baseline_hyp_sentence.append(vocab.idx2word[word_idx])\n",
    "    \n",
    "    ref_sentence = []\n",
    "    for word_idx in references[k]:\n",
    "        ref_sentence.append(vocab.idx2word[word_idx])\n",
    "        \n",
    "    img = imgs[0][k] \n",
    "#     print(img)\n",
    "#     print(imgs)\n",
    "    misc.imsave('img.jpg', img)\n",
    "  \n",
    "    img = misc.imread('img.jpg')\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "        \n",
    "    print('References: '+\" \".join(ref_sentence))\n",
    "    print(' ')\n",
    "    print('Baseline  : '+\" \".join(baseline_hyp_sentence))\n",
    "    print('GloVe     : '+\" \".join(glove_hyp_sentence))\n",
    "    print('BERT      : '+\" \".join(bert_hyp_sentence))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [] \n",
    "bert_hypotheses = []\n",
    "glove_hypotheses = [] \n",
    "baseline_hypotheses = [] \n",
    "all_imgs = []\n",
    "\n",
    "def compare_models():\n",
    "    \n",
    "    # load all pre-trained models\n",
    "    \n",
    "    encoder_glove = Encoder()\n",
    "    encoder_checkpoint = torch.load('./checkpoints_glove/encoder_glove',map_location='cpu')\n",
    "    encoder_glove.load_state_dict(encoder_checkpoint['model_state_dict'])\n",
    "    decoder_glove = Decoder(vocab_size=len(vocab),use_glove=True, use_bert=False)\n",
    "    decoder_optimizer = torch.optim.Adam(params=decoder_glove.parameters(),lr=decoder_lr)\n",
    "    decoder_checkpoint = torch.load('./checkpoints_glove/decoder_glove',map_location='cpu')\n",
    "    decoder_glove.load_state_dict(decoder_checkpoint['model_state_dict'])\n",
    "    decoder_optimizer.load_state_dict(decoder_checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    #########\n",
    "\n",
    "    encoder_bert = Encoder()\n",
    "    encoder_checkpoint = torch.load('./checkpoints_bert/encoder_bert',map_location='cpu')\n",
    "    encoder_bert.load_state_dict(encoder_checkpoint['model_state_dict'])\n",
    "    decoder_bert = Decoder(vocab_size=len(vocab),use_glove=False, use_bert=True)\n",
    "    decoder_optimizer = torch.optim.Adam(params=decoder_bert.parameters(),lr=decoder_lr)\n",
    "    decoder_checkpoint = torch.load('./checkpoints_bert/decoder_bert',map_location='cpu')\n",
    "    decoder_bert.load_state_dict(decoder_checkpoint['model_state_dict'])\n",
    "    decoder_optimizer.load_state_dict(decoder_checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    #########\n",
    "\n",
    "    encoder_baseline = Encoder()\n",
    "    encoder_checkpoint = torch.load('./checkpoints_baseline1/encoder_baseline',map_location='cpu')\n",
    "    encoder_baseline.load_state_dict(encoder_checkpoint['model_state_dict'])\n",
    "    decoder_baseline = Decoder(vocab_size=len(vocab),use_glove=False, use_bert=False)\n",
    "    decoder_optimizer = torch.optim.Adam(params=decoder_baseline.parameters(),lr=decoder_lr)\n",
    "    decoder_checkpoint = torch.load('./checkpoints_baseline1/decoder_baseline',map_location='cpu')\n",
    "    decoder_baseline.load_state_dict(decoder_checkpoint['model_state_dict'])\n",
    "    decoder_optimizer.load_state_dict(decoder_checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    print(\"Started Comparison...\")\n",
    "    decoder_bert.eval()\n",
    "    encoder_bert.eval()\n",
    "    decoder_glove.eval()\n",
    "    decoder_baseline.eval()\n",
    "\n",
    "    # Batches\n",
    "    for i, (imgs, caps, caplens) in enumerate(tqdm(val_loader)):\n",
    "        if i > 0:\n",
    "            break\n",
    "\n",
    "        imgs_jpg = imgs.numpy() \n",
    "        imgs_jpg = np.swapaxes(np.swapaxes(imgs_jpg, 1, 3), 1, 2)\n",
    "        \n",
    "        # Forward prop.\n",
    "        imgs = encoder_bert(imgs)\n",
    "        scores_bert, caps_sorted_bert,decode_lengths_bert , _ = decoder_bert(imgs, caps, caplens)\n",
    "        targets = caps_sorted_bert[:, 1:]\n",
    "        scores_glove, caps_sorted_glove, decode_lengths_glove, _ = decoder_glove(imgs, caps, caplens)\n",
    "        scores_baseline, caps_sorted_baseline, decode_lengths_baseline, _ = decoder_baseline(imgs, caps, caplens)\n",
    "\n",
    "         # References\n",
    "        for j in range(targets.shape[0]):\n",
    "            img_caps = targets[j].tolist() # validation dataset only has 1 unique caption per img\n",
    "            clean_cap = [w for w in img_caps if w not in [PAD, START, END]]  # remove pad, start, and end\n",
    "            img_captions = list(map(lambda c: clean_cap,img_caps))\n",
    "            references.append(clean_cap)\n",
    "\n",
    "        # Hypotheses\n",
    "        _, preds_bert = torch.max(scores_bert, dim=2)\n",
    "        _, preds_glove = torch.max(scores_glove, dim=2)\n",
    "        _, preds_baseline = torch.max(scores_baseline, dim=2)\n",
    "        preds_bert = preds_bert.tolist()\n",
    "        preds_glove = preds_glove.tolist()\n",
    "        preds_baseline = preds_baseline.tolist()\n",
    "        \n",
    "        temp_preds_bert = list()\n",
    "        temp_preds_glove = list()\n",
    "        temp_preds_baseline = list()\n",
    "        \n",
    "        for j, p in enumerate(preds_bert):\n",
    "            pred = preds_bert[j][:decode_lengths_bert[j]]\n",
    "            pred = [w for w in pred if w not in [PAD, START, END]]\n",
    "            temp_preds_bert.append(pred)  # remove pads, start, and end\n",
    "            \n",
    "            pred = preds_glove[j][:decode_lengths_glove[j]]\n",
    "            pred = [w for w in pred if w not in [PAD, START, END]]\n",
    "            temp_preds_glove.append(pred)  # remove pads, start, and end\n",
    "            \n",
    "            pred = preds_baseline[j][:decode_lengths_baseline[j]]\n",
    "            pred = [w for w in pred if w not in [PAD, START, END]]\n",
    "            temp_preds_baseline.append(pred)  # remove pads, start, and end\n",
    "            \n",
    "        bert_hypotheses.extend(temp_preds_bert)\n",
    "        glove_hypotheses.extend(temp_preds_glove)\n",
    "        baseline_hypotheses.extend(temp_preds_baseline)\n",
    "        all_imgs.append(imgs_jpg)\n",
    "        print(len(bert_hypotheses))\n",
    "#         print(glove_hypotheses)\n",
    "#         print(baseline_hypotheses)\n",
    "        \n",
    "        compare_sample(references,bert_hypotheses,glove_hypotheses, baseline_hypotheses, all_imgs, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_sample(references,bert_hypotheses,glove_hypotheses, baseline_hypotheses, all_imgs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open('/home/zshen/github/Image-Captions/data/annotations/captions_train2014.json', 'r') as f:\n",
    "    train = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [] \n",
    "test_references = []\n",
    "hypotheses = [] \n",
    "all_imgs = []\n",
    "all_alphas = []\n",
    "result =[]\n",
    "f = open('/home/zshen/github/Image-Captions/data/results/captions_val2014_emb_results.json', 'w')\n",
    "def validate():\n",
    "    print(\"Started validation...\")\n",
    "    decoder.eval()\n",
    "    encoder.eval()\n",
    "\n",
    "    losses = loss_obj()\n",
    "\n",
    "    num_batches = len(val_loader)\n",
    "    print('Batches',num_batches)\n",
    "    # Batches\n",
    "    for i, (imgs, caps, caplens,img_ids) in enumerate(tqdm(val_loader)):\n",
    "        if i > 0:\n",
    "            break\n",
    "\n",
    "        imgs_jpg = imgs.numpy() \n",
    "        imgs_jpg = np.swapaxes(np.swapaxes(imgs_jpg, 1, 3), 1, 2)\n",
    "        \n",
    "        # Forward prop.\n",
    "        imgs = encoder(imgs)\n",
    "        scores, caps_sorted, decode_lengths, alphas = decoder(imgs, caps, caplens)\n",
    "        targets = caps_sorted[:, 1:]\n",
    "\n",
    "        # Remove timesteps that we didn't decode at, or are pads\n",
    "        scores_packed = pack_padded_sequence(scores, decode_lengths, batch_first=True)[0]\n",
    "        targets_packed = pack_padded_sequence(targets, decode_lengths, batch_first=True)[0]\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(scores_packed, targets_packed)\n",
    "        loss += ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
    "\n",
    "        losses.update(loss.item(), sum(decode_lengths))\n",
    "\n",
    "         # References\n",
    "        for j in range(targets.shape[0]):\n",
    "            img_caps = targets[j].tolist() # validation dataset only has 1 unique caption per img\n",
    "            clean_cap = [w for w in img_caps if w not in [PAD, START, END]]  # remove pad, start, and end\n",
    "            img_captions = list(map(lambda c: clean_cap,img_caps))\n",
    "            test_references.append(clean_cap)\n",
    "            references.append(img_captions)\n",
    "\n",
    "        # Hypotheses\n",
    "        _, preds = torch.max(scores, dim=2)\n",
    "        preds = preds.tolist()\n",
    "        temp_preds = list()\n",
    "        for j, p in enumerate(preds):\n",
    "            pred = p[:decode_lengths[j]]\n",
    "            pred = [w for w in pred if w not in [PAD, START, END]]\n",
    "            temp_preds.append(pred)  # remove pads, start, and end\n",
    "        preds = temp_preds\n",
    "        hypotheses.extend(preds)\n",
    "        \n",
    "        all_alphas.append(alphas)\n",
    "        all_imgs.append(imgs_jpg)\n",
    "        \n",
    "        for j,img_id in enumerate(img_ids):\n",
    "            hyp_sentence = []\n",
    "            for word_idx in hypotheses[j]:\n",
    "                hyp_sentence.append(vocab.idx2word[word_idx])\n",
    "\n",
    "            sample = {}\n",
    "            sample[\"image_id\"] = img_id\n",
    "            sample[\"caption\"] = \" \".join(hyp_sentence)\n",
    "#             print('img_id:',img_id,'hyp_sentence:',hyp_sentence)\n",
    "            result.append(sample)\n",
    "#             print('Hypotheses: '+\" \".join(hyp_sentence))\n",
    "#             print('References: '+\" \".join(ref_sentence))\n",
    "    json.dump(result,f)    \n",
    "    print(\"Completed validation...\")\n",
    "\n",
    "    # [{\n",
    "# \"image_id\": int, \n",
    "#  \"caption\": str,\n",
    "# }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started validation...\n",
      "Batches 6333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zshen/.local/lib/python3.6/site-packages/ipykernel_launcher.py:18: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b615576bc245c2aecbc6012a483738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6333), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed validation...\n"
     ]
    }
   ],
   "source": [
    "validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in train.items():\n",
    "    print (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/zshen/github/Image-Captions/data/annotations/captions_val2014.json', 'r') as f:\n",
    "    val2014 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val2014['annotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in val2014.items():\n",
    "    print (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/zshen/github/Image-Captions/data/annotations/image_info_test2014.json', 'r') as f:\n",
    "    test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in test.items():\n",
    "    print (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['annotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val2014['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val2014['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['images'] = train['images']+val2014['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['annotations'] = train['annotations']+val2014['annotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/zshen/github/Image-Captions/data/annotations/captions_tain_val2014.json', 'w') as f:\n",
    "    json.dump(train,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/zshen/github/Image-Captions/data/annotations/captions_tain_val2014.json', 'r') as f:\n",
    "    trainval = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainval['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv /home/zshen/github/Image-Captions/data/annotations/captions_tain_val2014.json /home/zshen/github/Image-Captions/data/annotations/captions_train_val2014.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /home/zshen/github/Image-Captions/data/annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainval['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainval['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(bert_hypotheses,glove_hypotheses, baseline_hypotheses, img_ids, k,results_bert,results_glove,results_baseline):\n",
    "    sample_bert={}    \n",
    "    sample_glove={}    \n",
    "    sample_baseline={}    \n",
    "    sample_bert[\"image_id\"] = img_ids[k]\n",
    "    sample_glove[\"image_id\"] = img_ids[k]\n",
    "    sample_baseline[\"image_id\"] = img_ids[k]\n",
    "\n",
    "    bert_hyp_sentence = []\n",
    "    for word_idx in bert_hypotheses[k]:\n",
    "        bert_hyp_sentence.append(vocab.idx2word[word_idx])\n",
    "    \n",
    "    sample_bert[\"caption\"] =\" \".join(bert_hyp_sentence)\n",
    "    results_bert.append(sample_bert)\n",
    "    \n",
    "    glove_hyp_sentence = []\n",
    "    for word_idx in glove_hypotheses[k]:\n",
    "        glove_hyp_sentence.append(vocab.idx2word[word_idx])\n",
    "    sample_glove[\"caption\"] =\" \".join(glove_hyp_sentence)\n",
    "    results_glove.append(sample_glove)\n",
    "    \n",
    "    baseline_hyp_sentence = []\n",
    "    for word_idx in baseline_hypotheses[k]:\n",
    "        baseline_hyp_sentence.append(vocab.idx2word[word_idx])\n",
    "    sample_baseline[\"caption\"] =\" \".join(baseline_hyp_sentence)\n",
    "    results_baseline.append(sample_baseline)\n",
    "    \n",
    "#     ref_sentence = []\n",
    "#     for word_idx in references[k]:\n",
    "#         ref_sentence.append(vocab.idx2word[word_idx])\n",
    "\n",
    "        \n",
    "#     print('img_id:',img_ids[k])    \n",
    "#     print('References: '+\" \".join(ref_sentence))\n",
    "#     print(' ')\n",
    "#     print('Baseline  : '+\" \".join(baseline_hyp_sentence))\n",
    "#     print('GloVe     : '+\" \".join(glove_hyp_sentence))\n",
    "#     print('BERT      : '+\" \".join(bert_hyp_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [] \n",
    "bert_hypotheses = []\n",
    "glove_hypotheses = [] \n",
    "baseline_hypotheses = [] \n",
    "\n",
    "results_bert =[]\n",
    "results_glove =[]\n",
    "results_baseline =[]\n",
    "\n",
    "f_bert = open('/home/zshen/github/Image-Captions/data/results/captions_val2014_bert_results.json', 'w')\n",
    "f_glove = open('/home/zshen/github/Image-Captions/data/results/captions_val2014_glove_results.json', 'w')\n",
    "f_baseline = open('/home/zshen/github/Image-Captions/data/results/captions_val2014_baseline_results.json', 'w')\n",
    "\n",
    "def gen():\n",
    "    \n",
    "    # load all pre-trained models\n",
    "    \n",
    "    encoder_glove = Encoder()\n",
    "    encoder_checkpoint = torch.load('./checkpoints_glove/encoder_glove',map_location='cpu')\n",
    "    encoder_glove.load_state_dict(encoder_checkpoint['model_state_dict'])\n",
    "    decoder_glove = Decoder(vocab_size=len(vocab),use_glove=True, use_bert=False)\n",
    "    decoder_optimizer = torch.optim.Adam(params=decoder_glove.parameters(),lr=decoder_lr)\n",
    "    decoder_checkpoint = torch.load('./checkpoints_glove/decoder_glove',map_location='cpu')\n",
    "    decoder_glove.load_state_dict(decoder_checkpoint['model_state_dict'])\n",
    "    decoder_optimizer.load_state_dict(decoder_checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    #########\n",
    "\n",
    "    encoder_bert = Encoder()\n",
    "    encoder_checkpoint = torch.load('./checkpoints_bert2/encoder_bert',map_location='cpu')\n",
    "    encoder_bert.load_state_dict(encoder_checkpoint['model_state_dict'])\n",
    "    decoder_bert = Decoder(vocab_size=len(vocab),use_glove=False, use_bert=True)\n",
    "    decoder_optimizer = torch.optim.Adam(params=decoder_bert.parameters(),lr=decoder_lr)\n",
    "    decoder_checkpoint = torch.load('./checkpoints_bert2/decoder_bert',map_location='cpu')\n",
    "    decoder_bert.load_state_dict(decoder_checkpoint['model_state_dict'])\n",
    "    decoder_optimizer.load_state_dict(decoder_checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    #########\n",
    "\n",
    "    encoder_baseline = Encoder()\n",
    "    encoder_checkpoint = torch.load('./checkpoints_baseline1/encoder_baseline',map_location='cpu')\n",
    "    encoder_baseline.load_state_dict(encoder_checkpoint['model_state_dict'])\n",
    "    decoder_baseline = Decoder(vocab_size=len(vocab),use_glove=False, use_bert=False)\n",
    "    decoder_optimizer = torch.optim.Adam(params=decoder_baseline.parameters(),lr=decoder_lr)\n",
    "    decoder_checkpoint = torch.load('./checkpoints_baseline1/decoder_baseline',map_location='cpu')\n",
    "    decoder_baseline.load_state_dict(decoder_checkpoint['model_state_dict'])\n",
    "    decoder_optimizer.load_state_dict(decoder_checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    print(\"Started Comparison...\")\n",
    "    decoder_bert.eval()\n",
    "    encoder_bert.eval()\n",
    "    decoder_glove.eval()\n",
    "    decoder_baseline.eval()\n",
    "    \n",
    "    # Batches\n",
    "    \n",
    "    for i, (imgs, caps, caplens,img_ids) in enumerate(tqdm(val_loader)):\n",
    "#         if i > 0:\n",
    "#             break\n",
    "\n",
    "        imgs_jpg = imgs.numpy() \n",
    "        imgs_jpg = np.swapaxes(np.swapaxes(imgs_jpg, 1, 3), 1, 2)\n",
    "        \n",
    "        # Forward prop.\n",
    "        imgs = encoder_bert(imgs)\n",
    "        scores_bert, caps_sorted_bert,decode_lengths_bert , _ = decoder_bert(imgs, caps, caplens)\n",
    "        targets = caps_sorted_bert[:, 1:]\n",
    "        scores_glove, caps_sorted_glove, decode_lengths_glove, _ = decoder_glove(imgs, caps, caplens)\n",
    "        scores_baseline, caps_sorted_baseline, decode_lengths_baseline, _ = decoder_baseline(imgs, caps, caplens)\n",
    "\n",
    "         # References\n",
    "        for j in range(targets.shape[0]):\n",
    "            img_caps = targets[j].tolist() # validation dataset only has 1 unique caption per img\n",
    "            clean_cap = [w for w in img_caps if w not in [PAD, START, END]]  # remove pad, start, and end\n",
    "            img_captions = list(map(lambda c: clean_cap,img_caps))\n",
    "            references.append(clean_cap)\n",
    "\n",
    "        # Hypotheses\n",
    "        _, preds_bert = torch.max(scores_bert, dim=2)\n",
    "        _, preds_glove = torch.max(scores_glove, dim=2)\n",
    "        _, preds_baseline = torch.max(scores_baseline, dim=2)\n",
    "        preds_bert = preds_bert.tolist()\n",
    "        preds_glove = preds_glove.tolist()\n",
    "        preds_baseline = preds_baseline.tolist()\n",
    "        \n",
    "        temp_preds_bert = list()\n",
    "        temp_preds_glove = list()\n",
    "        temp_preds_baseline = list()\n",
    "        \n",
    "        for j, p in enumerate(preds_bert):\n",
    "            pred = preds_bert[j][:decode_lengths_bert[j]]\n",
    "            pred = [w for w in pred if w not in [PAD, START, END]]\n",
    "            temp_preds_bert.append(pred)  # remove pads, start, and end\n",
    "            \n",
    "            pred = preds_glove[j][:decode_lengths_glove[j]]\n",
    "            pred = [w for w in pred if w not in [PAD, START, END]]\n",
    "            temp_preds_glove.append(pred)  # remove pads, start, and end\n",
    "            \n",
    "            pred = preds_baseline[j][:decode_lengths_baseline[j]]\n",
    "            pred = [w for w in pred if w not in [PAD, START, END]]\n",
    "            temp_preds_baseline.append(pred)  # remove pads, start, and end\n",
    "            \n",
    "        bert_hypotheses.extend(temp_preds_bert)\n",
    "        glove_hypotheses.extend(temp_preds_glove)\n",
    "        baseline_hypotheses.extend(temp_preds_baseline)\n",
    "        \n",
    "        \n",
    "        for k,img_id in enumerate(img_ids):        \n",
    "            save_json(bert_hypotheses,glove_hypotheses, baseline_hypotheses, img_ids, k,results_bert,results_glove,results_baseline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Comparison...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zshen/.local/lib/python3.6/site-packages/ipykernel_launcher.py:57: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "812a3921f6764890a086f0cbb02334dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6333), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/zshen/github/Image-Captions/data/results/captions_val2014_bert_results.json', 'w') as f_bert:\n",
    "    json.dump(results_bert,f_bert)\n",
    "with open('/home/zshen/github/Image-Captions/data/results/captions_val2014_glove_results.json', 'w') as f_glove:\n",
    "    json.dump(results_glove,f_glove)\n",
    "with open('/home/zshen/github/Image-Captions/data/results/captions_val2014_baseline_results.json', 'w') as f_baseline:\n",
    "    json.dump(results_baseline,f_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202654"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/zshen/github/Image-Captions/data/results/captions_val2014_bert_results.json', 'w') as f_bert:\n",
    "    json.dump(results_bert,f_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mcheckpoints\u001b[0m/                  \u001b[01;32mdata_loader.py\u001b[0m*      \u001b[01;32mprocessData.py\u001b[0m*\r\n",
      "\u001b[01;34mcheckpoints_baseline\u001b[0m/         \u001b[01;34meval_results\u001b[0m/        \u001b[01;34m__pycache__\u001b[0m/\r\n",
      "\u001b[01;34mcheckpoints_baseline1\u001b[0m/        gen_final.ipynb      README.md\r\n",
      "\u001b[01;34mcheckpoints_bert\u001b[0m/             gen_final.py         requirements.txt\r\n",
      "\u001b[01;34mcheckpoints_bert2\u001b[0m/            gen.ipynb            \u001b[01;34mresults\u001b[0m/\r\n",
      "\u001b[01;34mcheckpoints_bert3\u001b[0m/            \u001b[01;34mglove.6B\u001b[0m/            test2json.ipynb\r\n",
      "\u001b[01;34mcheckpoints_bert_train_val1\u001b[0m/  glove_embeds.py      train_baseline.py\r\n",
      "\u001b[01;34mcheckpoints_glove\u001b[0m/            \u001b[01;35mimg.jpg\u001b[0m              train_bert.py\r\n",
      "\u001b[01;34mcoco-caption\u001b[0m/                 main_notebook.ipynb  train_glove.py\r\n",
      "\u001b[01;34mdata\u001b[0m/                         main.py\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emb",
   "language": "python",
   "name": "emb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
