{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"  # specify which GPU(s) to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from data_loader import get_loader\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from processData import Vocabulary\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import skimage.transform\n",
    "import argparse\n",
    "from scipy.misc import imread, imresize\n",
    "from PIL import Image\n",
    "import matplotlib.image as mpimg\n",
    "from IPython import display\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "from scipy import misc\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Encoder RASNET CNN - pretrained\n",
    "#####################################\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        resnet = models.resnet101(pretrained=True)\n",
    "        self.resnet = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((14, 14))\n",
    "\n",
    "    def forward(self, images):\n",
    "        out = self.adaptive_pool(self.resnet(images))\n",
    "        # batch_size, img size, imgs size, 2048\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Attention Decoder\n",
    "####################\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, use_glove, use_bert):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.encoder_dim = 2048\n",
    "        self.attention_dim = 512\n",
    "        self.use_bert = use_bert\n",
    "        if use_glove:\n",
    "            self.embed_dim = 300\n",
    "        elif use_bert:\n",
    "            self.embed_dim = 768\n",
    "        else:\n",
    "            self.embed_dim = 512\n",
    "            \n",
    "        self.decoder_dim = 512\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = 0.5\n",
    "        \n",
    "        # soft attention\n",
    "        self.enc_att = nn.Linear(2048, 512)\n",
    "        self.dec_att = nn.Linear(512, 512)\n",
    "        self.att = nn.Linear(512, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        # decoder layers\n",
    "        self.dropout = nn.Dropout(p=self.dropout)\n",
    "        self.decode_step = nn.LSTMCell(self.embed_dim + self.encoder_dim, self.decoder_dim, bias=True)\n",
    "        self.h_lin = nn.Linear(self.encoder_dim, self.decoder_dim)\n",
    "        self.c_lin = nn.Linear(self.encoder_dim, self.decoder_dim)\n",
    "        self.f_beta = nn.Linear(self.decoder_dim, self.encoder_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc = nn.Linear(self.decoder_dim, self.vocab_size)\n",
    "\n",
    "        # init variables\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "        \n",
    "        if not use_bert:\n",
    "            self.embedding = nn.Embedding(vocab_size, self.embed_dim)\n",
    "            self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "            # load Glove embeddings\n",
    "            if use_glove:\n",
    "                self.embedding.weight = nn.Parameter(glove_vectors)\n",
    "\n",
    "            # always fine-tune embeddings (even with GloVe)\n",
    "            for p in self.embedding.parameters():\n",
    "                p.requires_grad = True\n",
    "            \n",
    "\n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):    \n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(-1)\n",
    "        vocab_size = self.vocab_size\n",
    "        dec_len = [x-1 for x in caption_lengths]\n",
    "        max_dec_len = max(dec_len)\n",
    "\n",
    "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "\n",
    "        if not self.use_bert:\n",
    "            embeddings = self.embedding(encoded_captions)\n",
    "        elif self.use_bert:\n",
    "            embeddings = []\n",
    "            for cap_idx in  encoded_captions:\n",
    "                \n",
    "                # padd caption to correct size\n",
    "                while len(cap_idx) < max_dec_len:\n",
    "                    cap_idx.append(PAD)\n",
    "                    \n",
    "                cap = ' '.join([vocab.idx2word[word_idx.item()] for word_idx in cap_idx])\n",
    "                cap = u'[CLS] '+cap\n",
    "                \n",
    "                tokenized_cap = tokenizer.tokenize(cap)                \n",
    "                indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_cap)\n",
    "                tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    encoded_layers, _ = model(tokens_tensor)\n",
    "\n",
    "                bert_embedding = encoded_layers[11].squeeze(0)\n",
    "                \n",
    "                split_cap = cap.split()\n",
    "                tokens_embedding = []\n",
    "                j = 0\n",
    "\n",
    "                for full_token in split_cap:\n",
    "                    curr_token = ''\n",
    "                    x = 0\n",
    "                    for i,_ in enumerate(tokenized_cap[1:]): # disregard CLS\n",
    "                        token = tokenized_cap[i+j]\n",
    "                        piece_embedding = bert_embedding[i+j]\n",
    "                        \n",
    "                        # full token\n",
    "                        if token == full_token and curr_token == '' :\n",
    "                            tokens_embedding.append(piece_embedding)\n",
    "                            j += 1\n",
    "                            break\n",
    "                        else: # partial token\n",
    "                            x += 1\n",
    "                            \n",
    "                            if curr_token == '':\n",
    "                                tokens_embedding.append(piece_embedding)\n",
    "                                curr_token += token.replace('#', '')\n",
    "                            else:\n",
    "                                tokens_embedding[-1] = torch.add(tokens_embedding[-1], piece_embedding)\n",
    "                                curr_token += token.replace('#', '')\n",
    "                                \n",
    "                                if curr_token == full_token: # end of partial\n",
    "                                    j += x\n",
    "                                    break                            \n",
    "                \n",
    "                               \n",
    "                cap_embedding = torch.stack(tokens_embedding)\n",
    "\n",
    "                embeddings.append(cap_embedding)\n",
    "                \n",
    "            embeddings = torch.stack(embeddings)\n",
    "\n",
    "        # init hidden state\n",
    "        avg_enc_out = encoder_out.mean(dim=1)\n",
    "        h = self.h_lin(avg_enc_out)\n",
    "        c = self.c_lin(avg_enc_out)\n",
    "\n",
    "        predictions = torch.zeros(batch_size, max_dec_len, vocab_size)\n",
    "        alphas = torch.zeros(batch_size, max_dec_len, num_pixels)\n",
    "\n",
    "        for t in range(max(dec_len)):\n",
    "            batch_size_t = sum([l > t for l in dec_len ])\n",
    "            \n",
    "            # soft-attention\n",
    "            enc_att = self.enc_att(encoder_out[:batch_size_t])\n",
    "            dec_att = self.dec_att(h[:batch_size_t])\n",
    "            att = self.att(self.relu(enc_att + dec_att.unsqueeze(1))).squeeze(2)\n",
    "            alpha = self.softmax(att)\n",
    "            attention_weighted_encoding = (encoder_out[:batch_size_t] * alpha.unsqueeze(2)).sum(dim=1)\n",
    "        \n",
    "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            \n",
    "            batch_embeds = embeddings[:batch_size_t, t, :]  \n",
    "            cat_val = torch.cat([batch_embeds.double(), attention_weighted_encoding.double()], dim=1)\n",
    "            \n",
    "            h, c = self.decode_step(cat_val.float(),(h[:batch_size_t].float(), c[:batch_size_t].float()))\n",
    "            preds = self.fc(self.dropout(h))\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "            alphas[:batch_size_t, t, :] = alpha\n",
    "            \n",
    "        # preds, sorted capts, dec lens, attention wieghts\n",
    "        return predictions, encoded_captions, dec_len, alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "class loss_obj(object):\n",
    "    def __init__(self):\n",
    "        self.avg = 0.\n",
    "        self.sum = 0.\n",
    "        self.count = 0.\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "PAD = 0\n",
    "START = 1\n",
    "END = 2\n",
    "UNK = 3\n",
    "\n",
    "grad_clip = 5.\n",
    "num_epochs = 1\n",
    "batch_size = 32\n",
    "decoder_lr = 0.0004\n",
    "# Load vocabulary wrapper\n",
    "with open('data/vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "# load data\n",
    "train_loader = get_loader('train', vocab, batch_size)\n",
    "val_loader = get_loader('val', vocab, batch_size)\n",
    "test_loader = get_loader('test', vocab, batch_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(bert_hypotheses,img_ids, k,results_bert):\n",
    "    sample_bert={}      \n",
    "    sample_bert[\"image_id\"] = img_ids[k]\n",
    "\n",
    "\n",
    "    bert_hyp_sentence = []\n",
    "    for word_idx in bert_hypotheses[k]:\n",
    "        bert_hyp_sentence.append(vocab.idx2word[word_idx])\n",
    "    \n",
    "    sample_bert[\"caption\"] =\" \".join(bert_hyp_sentence)\n",
    "    results_bert.append(sample_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_hypotheses = []\n",
    "\n",
    "results_bert =[]\n",
    "\n",
    "\n",
    "def gen():\n",
    "    \n",
    "    # load  pre-trained model\n",
    "    \n",
    "\n",
    "    encoder_bert = Encoder()\n",
    "    encoder_checkpoint = torch.load('./checkpoints_bert2/encoder_bert')\n",
    "    encoder_bert.load_state_dict(encoder_checkpoint['model_state_dict'])\n",
    "    decoder_bert = Decoder(vocab_size=len(vocab),use_glove=False, use_bert=True)\n",
    "    decoder_optimizer = torch.optim.Adam(params=decoder_bert.parameters(),lr=decoder_lr)\n",
    "    decoder_checkpoint = torch.load('./checkpoints_bert2/decoder_bert')\n",
    "    decoder_bert.load_state_dict(decoder_checkpoint['model_state_dict'])\n",
    "    decoder_optimizer.load_state_dict(decoder_checkpoint['optimizer_state_dict'])\n",
    "\n",
    "\n",
    "    print(\"Started Comparison...\")\n",
    "    decoder_bert.eval()\n",
    "    encoder_bert.eval()\n",
    "\n",
    "    \n",
    "    # Batches\n",
    "    \n",
    "    for i, (imgs, caps, caplens,img_ids) in enumerate(tqdm(val_loader)):\n",
    "#         if i > 0:\n",
    "#             break\n",
    "\n",
    "        imgs_jpg = imgs.numpy() \n",
    "        imgs_jpg = np.swapaxes(np.swapaxes(imgs_jpg, 1, 3), 1, 2)\n",
    "        \n",
    "        # Forward prop.\n",
    "        imgs = encoder_bert(imgs)\n",
    "        scores_bert, caps_sorted_bert,decode_lengths_bert , _ = decoder_bert(imgs, caps, caplens)\n",
    "        targets = caps_sorted_bert[:, 1:]\n",
    "\n",
    "\n",
    "        # Hypotheses\n",
    "        _, preds_bert = torch.max(scores_bert, dim=2)\n",
    "\n",
    "        preds_bert = preds_bert.tolist()\n",
    "\n",
    "        \n",
    "        temp_preds_bert = list()\n",
    "        for j, p in enumerate(preds_bert):\n",
    "            pred = preds_bert[j][:decode_lengths_bert[j]]\n",
    "            pred = [w for w in pred if w not in [PAD, START, END]]\n",
    "            temp_preds_bert.append(pred)  # remove pads, start, and end\n",
    "            \n",
    "        bert_hypotheses.extend(temp_preds_bert)\n",
    "        \n",
    "        \n",
    "        for k,img_id in enumerate(img_ids):        \n",
    "            save_json(bert_hypotheses, img_ids, k,results_bert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0c2066717c66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-65f6849fd1b8>\u001b[0m in \u001b[0;36mgen\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mencoder_bert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mencoder_checkpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./checkpoints_bert2/encoder_bert'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mencoder_bert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_checkpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Encoder' is not defined"
     ]
    }
   ],
   "source": [
    "gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/zshen/github/Image-Captions/data/results/captions_test2014_bert_results.json', 'w') as f_bert:\n",
    "    json.dump(results_bert,f_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emb",
   "language": "python",
   "name": "emb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
